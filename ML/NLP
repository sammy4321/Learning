Basic Concepts : 
1. Tokenization : Tokenization is the process of breaking down a text into smaller units, called tokens. These tokens can be words, phrases, symbols, or other meaningful elements.
2. Stemming : Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. Example: The words "running," "runner," and "ran" might all be reduced to the stem "run."
3. Lemmatization is a more advanced process than stemming and involves reducing words to their base or dictionary form, called a lemma. Unlike stemming, lemmatization considers the context and the part of speech of a word to ensure that the base form is a valid word in the language. Example: The words "running" and "ran" would be lemmatized to "run." However, lemmatization would correctly identify "running" as a verb and "ran" as its past tense.
4. Stopwords : Stopwords are words that are filtered out before processing of natural language data. Examples of stopwords include articles, prepositions, pronouns, and common nouns. Eg : a, the, is, are, of, in, on, at, from, and so on.
5. Part of Speech Tagging : Part of speech tagging is the process of marking up a word in a text with its part of speech. Eg : Noun, Verb, Adjective, Adverb, Pronoun, Preposition, Conjunction, Interjection, Determiner, Article, and so on.
6. Named Entity Recognition : Named entity recognition (NER) is the task of classifying named entities in text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. 